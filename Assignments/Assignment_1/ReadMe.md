# MLOps Assignment-1: Foundations of MLOps

This repository showcases the implementation of MLOps Assignment-1, focusing on building foundational skills in modern MLOps practices. 
It provides a comprehensive demonstration of CI/CD pipelines, experiment tracking, data versioning, model experimentation, packaging, and optional deployment techniques.  

**Author**: Group_37  
**Institution**: BITS Pilani (Work Integrated Learning Programmes)  
**Course**: M.Tech. in Artificial Intelligence & Machine Learning
**Assignment**: MLOps Assignment-1 [Total Marks: 25]  

---

## Table of Contents
- [Project Overview](#project-overview)
- [Deliverables Checklist](#deliverables-checklist)
- [Setup and Usage Instructions](#setup-and-usage-instructions)
  - [Requirements](#requirements)
  - [Project Structure](#project-structure)
  - [Run Instructions](#run-instructions)
- [Module-Wise Breakdown](#module-wise-breakdown)
  - [Module 1: CI/CD Pipeline and Version Control](#module-1-cicd-pipeline-and-version-control)
  - [Module 2: Experiment Tracking and Data Versioning](#module-2-experiment-tracking-and-data-versioning)
  - [Module 3: Model Experimentation and Packaging](#module-3-model-experimentation-and-packaging)
- [Technologies and Tools Used](#technologies-and-tools-used)

---

## Project Overview
This project is an implementation of Assignment-1 for MLOps Foundations. The objective is to demonstrate understanding and hands-on experience with the key components of an MLOps workflow. It covers:

1. Automating workflows with a CI/CD pipeline using GitHub Actions.  
2. Tracking machine learning experiments using MLflow.  
3. Versioning data with DVC to maintain reproducibility.  
4. Hyperparameter tuning using Optuna.  
5. Packaging and running the model as a web service with Flask and Docker.  

---

## Deliverables Checklist

### **Essential Deliverables**:
1. CI/CD Pipeline setup using GitHub Actions.
2. Experiment tracking logs generated by MLflow.
3. DVC setup with tracked dataset versions.
4. Dockerized Flask application to serve the model.
5. Code repository with branching, merging, and pull requests.  

### **Optional Deliverables (Module 4)**:
1. Kubernetes deployment using a managed cloud service (AWS, Azure, or GCP).  
2. Helm charts for deployment orchestration.  

---

## Setup and Usage Instructions

### Requirements
To run this project, ensure the following are installed:  
- Python 3.8 or later  
- Docker  
- Git and DVC  
- MLflow  
- Optuna  

Install Python dependencies:  
```bash
pip install -r src/requirements.txt
```

### Project Structure
```
├── .github/
│   ├── workflows/
│   │   ├── main.yml          # CI/CD pipeline configuration
├── data/
│   ├── dataset.csv           # Example dataset (managed with DVC)
├── models/
│   ├── model.pkl             # Best-trained model
├── src/
│   ├── train.py              # Model training and evaluation
│   ├── app.py                # Flask application to serve model predictions
│   ├── requirements.txt      # Python dependencies
├── Dockerfile                # Docker configuration for model serving
├── ReadMe.md                 # Project documentation (this file)
```

### Run Instructions
#### 1. Run the CI/CD Pipeline
- Push changes to GitHub, and GitHub Actions will trigger the pipeline automatically.  
- The pipeline runs include:  
  - **Linting**: Ensure code quality with flake8.  
  - **Testing**: Verify functionality using pytest.  
  - **Deployment**: Automate container builds.

#### 2. Experiment Tracking with MLflow
Run the following commands to track experiments:  
```bash
mlflow ui
python src/train.py
```
Access the MLflow UI at `http://localhost:5000`.

#### 3. Dockerize and Run the Flask App
Build the Docker container and run the model-serving application:  
```bash
docker build -t mlops-model .
docker run -p 5000:5000 mlops-model
```

#### 4. Data Management with DVC
DVC is used to version control datasets:  
- To add a dataset:  
  ```bash
  dvc add data/dataset.csv
  ```
- To pull data from the remote repository:  
  ```bash
  dvc pull
  ```
- To revert to a previous dataset version:  
  ```bash
  dvc checkout
  ```

---

## Module-Wise Breakdown

### Module 1: CI/CD Pipeline and Version Control
- **Objective**: Automate workflows with GitHub Actions and use Git for version control.  
- **Details**:  
  - CI/CD pipeline YAML file located in `.github/workflows/main.yml`.  
  - Linting and testing stages verify quality before merging to the `main` branch.  
  - Demonstrates branching, merging, and pull requests.  

### Module 2: Experiment Tracking and Data Versioning
- **Objective**: Use MLflow to track model metrics and DVC for dataset versioning.  
- **Experiment Tracking**:  
  - Logged metrics, hyperparameters, and results for three training runs.  
- **Data Versioning**:  
  - Tracks dataset versions and allows for reverting to earlier versions.  

### Module 3: Model Experimentation and Packaging
- **Objective**: Perform hyperparameter tuning and serve the best model via Flask and Docker.  
- **Details**:  
  - **Tuning**: Performed with Optuna for optimal hyperparameters.  
  - **Packaging**: Dockerized Flask app serves predictions at `http://localhost:5000`.

---

## Technologies and Tools Used

| Component                 | Tool/Technology  |
|---------------------------|------------------|
| CI/CD                     | GitHub Actions   |
| Version Control           | Git and DVC      |
| Experiment Tracking       | MLflow           |
| Hyperparameter Tuning     | Optuna           |
| Model Packaging           | Docker, Flask    |
| Cloud Deployment (Optional)| Kubernetes (GKE, AKS, AWS ECS) |

---

## Author
**Surendra**  
M.Tech. in Data Science & Engineering and AIML  
[GitHub Profile](https://github.com/iSPYadav01) | [Portfolio](https://ispyadav01.github.io/Portfolio/)  

---
